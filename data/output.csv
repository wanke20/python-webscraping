Section,Content
Time & Location:,
Staff,
Course Description,"Introduction to deep learning, including the statistical learning framework, empirical risk minimization, loss function selection, fully connected layers, convolutional layers, pooling layers, batch normalization, multi-layer perceptrons, convolutional neural networks, autoencoders, U-nets, residual networks, gradient descent, stochastic gradient descent, backpropagation, autograd, visualization of neural network features, robustness and adversarial examples, interpretability, continual learning, and applications in computer vision and natural language processing. Assumes students already have a basic knowledge of machine learning, optimization, linear algebra, and statistics.OverviewThe learning objectives of this course are that students should:be able to train neural networks at a variety of tasksbe able to select appropriate neural network architectures for a variety of taskshave read and be able to discuss the contents and contributions of important papers in the fieldhave a basic theoretical understanding of the tools of deep learningCourse Structure and Etiquette:Because of social distancing requirements, this course will be conducted primarily over Zoom.  The instructor will be screensharing papers and/or digital notes from the classroom and will facilitate classwide discussion over Zoom.  Participation in the class discussion is expected, and everyone will get a chance to speak.  Lectures will be recorded and posted on Canvas.  
During class, students should be fully engaged with the class, to the best of their ability.  In particular, students should be willing with the whole class their responses to prepared questions.  Students are encouraged to leave their video connection on for the majority of class.  Participants should mute themselves when they are not speaking.Student Work:Students will be expected to complete the following work:Each week, students will be given a set of questions to answer in writing about that week's content.  Students are expected to submit the answers to these questions to Gradescope by 2:30 PM on the relevant day of class in order to ensure they are prepared to engage in the class discussion.  Students will work in small groups.Students are expected to participate in the whole-class discussions by presenting prepared responses and answering spontaneous questions.If a student is unable to participate in a class, for example due to an absence, then they are expected to submit to Gradescope a typed or handwritten summary of the course discussion from that day.  This is not intended to be a transcript of every word that was said; it is meant to concisely convey all of the important ideas that were discussed.There will be three homework assignments throughout the semester.  They will involve both pencil-and-paper work and computation involving training neural networks.  The results will be submitted to gradescope in the form of a pdf.Students will complete a project.  They will reproduce an empirical observation in a paper of their choice by independently recreating and executing code to train and test that network.  You may not use the code from the specific paper you are replicating, but you may use code from other papers.Course grades:Course grades will be based on: 30% Preparation questions for classroom, 20% Participation, 30% HWs, 20% Project.Letter grades will be assigned on the following scale: 93%+ A, 90-92% A-, 87-89 B+, 83-87% B, 80-82% B-, 77-79 C+, 73-77% C, 70-72% C-,60-70% D, 0-59% F.Course Etiquette:PrerequisitesStudents are expected to have experience with a class in Machine Learning.   The class will assume students are comfortable with some linear algebra, probability, and statistics.  Some experience with neural networks, python, PyTorch/TensorFlow will be helpful but can be acquired while taking this class.  If you do not have experience with PyTorch, a good resource is the bookDeep Learning with Pytorch.DayDateClass Discussion Will Be On:1W 1/20Machine Learning Review(Notes)A DARPA Perspective on Artificial IntelligencePreparation Questions for Class2M 1/25Machine Learning Review(Notes) (Annotated)3W 1/27Deep LearningUnderstanding deep learning requires rethinking generalizationPreparation Questions for Class(tex)4M 2/1Architectural Elements of Neural Networks. (Notes) (Annotated).5W 2/3Visualizing and Understanding Convolutional NetworksVisualizing Higher-Layer Features of a Deep NetworkPreparation Questions for Class(tex)6M 2/8Gradient Descent and Stochastic Gradient Descent(Notes) (Annotated)7W 2/10HW 1 (pdf,tex) DUE (F 2/12)Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate ShiftHow Does Batch Normalization Help Optimization?Preparation Questions for Class(tex)-M 2/15No Class - Holiday8W 2/17Deep Learning Book - Chapter 8Adam: A Method for Stochastic OptimizationPreparation Questions for Class(tex)Annotated Notes9M 2/22Neural Network Architectures for Images(Notes)Annotated Notes10W 2/24Deep Residual Learning for Image Recognition(ResNets)ImageNet Classification with Deep Convolutional Neural Networks(AlexNet)Preparation Questions for Class(tex)Annotated Notes11M 3/1Watch DeepLearningAI videos on Object Localization and Detection:1,2,3,4,6,7,8,9,10,You Only Look Once: Unified, Real-Time Object DetectionClass notes (unannotated,annotated)12W 3/3HW 2 (pdf,tex) DUE (F 3/5)Attention Is All You NeedLanguage Models are Few-Shot LearnersPreparation questions for class(tex)Class notes (annotated)13M 3/8Adversarial Examples for Deep Neural Networks(Notes)Class notes (unannotated,annotated)14W 3/10Explaining and Harnessing Adversarial ExamplesRobust Physical-World Attacks on Deep Learning ModelsPreparation Questions for Class(tex)Class notes (annotated)15M 3/15""Why Should I Trust You?"" Explaining the Predictions of Any ClassifierWatchthis video explanationby one of the authors.A Survey on Neural Network Interpretability(optional)Class notes (unannotated, annotated)16W 3/17Guest Lecture - Generative Priors17M 3/22Continual Learning and Catastrophic Forgetting(Notes)Class notes (unannotated,annotated)-W 3/24HW 3 (pdf,tex) DUE (F 3/26)No class - University Care Day18M 3/29Project Planning Document (pdf,tex) Due (M 3/29)Automatic Differentiation, BackpropagationWatch thisvideoof automatic differentiation.Watchthis lecture(from start until time 38:30) on backpropagation of neural networks.Class notes (annotated)19W 3/31Overcoming catastrophic forgetting in neural networksPreparation Questions for Class(tex)Class notes (unannotated,annotated)20M 4/5Generative Adversarial Networks(notes)Class notes (unannotated,annotated)21W 4/7Variational Autoencoders(notes)-M 4/12No class - University Care Day22W 4/14Final Discussion23M 4/19Project Presentations24M 4/21Project Presentations"
Overview,be able to train neural networks at a variety of tasks | be able to select appropriate neural network architectures for a variety of tasks | have read and be able to discuss the contents and contributions of important papers in the field | have a basic theoretical understanding of the tools of deep learning | be able to train neural networks at a variety of tasks | be able to select appropriate neural network architectures for a variety of tasks | have read and be able to discuss the contents and contributions of important papers in the field | have a basic theoretical understanding of the tools of deep learning
Course Structure and Etiquette:,
Student Work:,"Each week, students will be given a set of questions to answer in writing about that week's content.  Students are expected to submit the answers to these questions to Gradescope by 2:30 PM on the relevant day of class in order to ensure they are prepared to engage in the class discussion.  Students will work in small groups. | Students are expected to participate in the whole-class discussions by presenting prepared responses and answering spontaneous questions. | If a student is unable to participate in a class, for example due to an absence, then they are expected to submit to Gradescope a typed or handwritten summary of the course discussion from that day.  This is not intended to be a transcript of every word that was said; it is meant to concisely convey all of the important ideas that were discussed. | There will be three homework assignments throughout the semester.  They will involve both pencil-and-paper work and computation involving training neural networks.  The results will be submitted to gradescope in the form of a pdf. | Students will complete a project.  They will reproduce an empirical observation in a paper of their choice by independently recreating and executing code to train and test that network.  You may not use the code from the specific paper you are replicating, but you may use code from other papers. | Each week, students will be given a set of questions to answer in writing about that week's content.  Students are expected to submit the answers to these questions to Gradescope by 2:30 PM on the relevant day of class in order to ensure they are prepared to engage in the class discussion.  Students will work in small groups. | Students are expected to participate in the whole-class discussions by presenting prepared responses and answering spontaneous questions. | If a student is unable to participate in a class, for example due to an absence, then they are expected to submit to Gradescope a typed or handwritten summary of the course discussion from that day.  This is not intended to be a transcript of every word that was said; it is meant to concisely convey all of the important ideas that were discussed. | There will be three homework assignments throughout the semester.  They will involve both pencil-and-paper work and computation involving training neural networks.  The results will be submitted to gradescope in the form of a pdf. | Students will complete a project.  They will reproduce an empirical observation in a paper of their choice by independently recreating and executing code to train and test that network.  You may not use the code from the specific paper you are replicating, but you may use code from other papers."
Course grades:,
Course Etiquette:,
Prerequisites," | Machine Learning Review(Notes) | Preparation Questions for Class | Machine Learning Review(Notes) (Annotated) |  | Preparation Questions for Class(tex) | Visualizing and Understanding Convolutional Networks | Visualizing Higher-Layer Features of a Deep Network | Preparation Questions for Class(tex) | How Does Batch Normalization Help Optimization? | Preparation Questions for Class(tex) | Deep Learning Book - Chapter 8 | Adam: A Method for Stochastic Optimization | Preparation Questions for Class(tex) | Annotated Notes | Annotated Notes |  | Preparation Questions for Class(tex) | Annotated Notes | Watch DeepLearningAI videos on Object Localization and Detection:1,2,3,4,6,7,8,9,10, | You Only Look Once: Unified, Real-Time Object Detection | Class notes (unannotated,annotated) | Attention Is All You Need | Language Models are Few-Shot Learners | Preparation questions for class(tex) | Class notes (annotated) | Class notes (unannotated,annotated) |  | Preparation Questions for Class(tex) | Class notes (annotated) | ""Why Should I Trust You?"" Explaining the Predictions of Any Classifier | Watchthis video explanationby one of the authors. | A Survey on Neural Network Interpretability(optional) | Class notes (unannotated, annotated) | Class notes (unannotated,annotated) | Watch thisvideoof automatic differentiation. | Watchthis lecture(from start until time 38:30) on backpropagation of neural networks. | Class notes (annotated) |  | Preparation Questions for Class(tex) | Class notes (unannotated,annotated) | Class notes (unannotated,annotated)"
